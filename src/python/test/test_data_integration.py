#!/usr/bin/env python
# coding: utf-8
"""
DekDataset - Test Data Integration

This script tests the integration between data_utils.py and generate_dataset.py
to ensure that data cleaning, analysis, and Hugging Face Hub integration work as expected.
"""

import os
import json
import argparse
from datetime import datetime
from data_utils import (
    clean_text, analyze_dataset, plot_word_cloud,
    plot_category_distribution, plot_length_distribution,
    plot_word_frequency
)
from colorama import Fore, Style, init

# Initialize colorama
init()

def test_data_cleaning():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô data_utils.py"""
    print(f"{Fore.CYAN}[TEST] Testing data cleaning functions...{Style.RESET_ALL}")
    
    test_cases = [
        {
            "input": "<p>‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠<b>‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö</b></p> ‡∏ó‡∏µ‡πà‡∏°‡∏µ  HTML ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ http://example.com",
            "expected": "‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏ó‡∏µ‡πà‡∏°‡∏µ HTML ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥",
            "options": {"remove_html": True, "remove_urls": True, "fix_spacing": True}
        },
        {
            "input": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ emoji üòä ‡πÅ‡∏•‡∏∞ üë®‚Äçüë©‚Äçüëß‚Äçüë¶ ‡∏Ñ‡∏ß‡∏£‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ",
            "expected": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ emoji üòä ‡πÅ‡∏•‡∏∞ üë®‚Äçüë©‚Äçüëß‚Äçüë¶ ‡∏Ñ‡∏ß‡∏£‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ",
            "options": {"remove_emojis": False}
        },
        {
            "input": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ emoji üòä ‡πÅ‡∏•‡∏∞ üë®‚Äçüë©‚Äçüëß‚Äçüë¶ ‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏π‡∏Å‡∏•‡∏ö",
            "expected": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ emoji  ‡πÅ‡∏•‡∏∞  ‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏π‡∏Å‡∏•‡∏ö",
            "options": {"remove_emojis": True}
        },
        {
            "input": "‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå http://example.com ‡πÅ‡∏•‡∏∞ https://www.test.co.th",
            "expected": "‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå  ‡πÅ‡∏•‡∏∞ ",
            "options": {"remove_urls": True}
        },
        {
            "input": "‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå http://example.com ‡πÅ‡∏•‡∏∞ https://www.test.co.th",
            "expected": "‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå http://example.com ‡πÅ‡∏•‡∏∞ https://www.test.co.th",
            "options": {"remove_urls": False}
        },
        {
            "input": "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢  ‡πÄ‡πÄ‡∏•‡∏∞  ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á",
            "expected": "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÅ‡∏•‡∏∞ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á",
            "options": {"normalize_thai": True, "fix_spacing": True}
        }
    ]
    
    all_passed = True
    for i, test in enumerate(test_cases):
        result = clean_text(test["input"], test["options"])
        if result == test["expected"]:
            print(f"{Fore.GREEN}[PASS] Test case {i+1} passed{Style.RESET_ALL}")
        else:
            print(f"{Fore.RED}[FAIL] Test case {i+1} failed{Style.RESET_ALL}")
            print(f"Expected: {test['expected']}")
            print(f"Got: {result}")
            all_passed = False
    
    if all_passed:
        print(f"{Fore.GREEN}[SUCCESS] All data cleaning tests passed!{Style.RESET_ALL}")
    else:
        print(f"{Fore.RED}[ERROR] Some data cleaning tests failed.{Style.RESET_ALL}")
    
    return all_passed

def test_dataset_analysis():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå dataset ‡πÉ‡∏ô data_utils.py"""
    print(f"{Fore.CYAN}[TEST] Testing dataset analysis functions...{Style.RESET_ALL}")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á sample dataset
    sample_data = [
        {
            "id": "test-1",
            "content": {
                "text": "‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå dataset ‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
            },
            "metadata": {"source": "test"}
        },
        {
            "id": "test-2",
            "content": {
                "text": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏£‡∏Å‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥‡πÜ ‡πÄ‡∏ä‡πà‡∏ô ‡∏ó‡∏î‡∏™‡∏≠‡∏ö ‡∏ó‡∏î‡∏™‡∏≠‡∏ö"
            },
            "metadata": {"source": "test"}
        },
        {
            "id": "test-3",
            "content": {
                "text": "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°"
            },
            "metadata": {"source": "test"}
        }
    ]
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    try:
        analysis = analyze_dataset(sample_data, field_path="content.text")
        print(f"{Fore.GREEN}[SUCCESS] Dataset analysis successful!{Style.RESET_ALL}")
        print(f"Total entries: {analysis['total_entries']}")
        print(f"Total words: {analysis['word_stats']['total_words']}")
        print(f"Unique words: {analysis['word_stats']['unique_words']}")
        print(f"Average text length: {analysis['length_stats']['avg']:.2f} characters")
        
        # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
        try:
            test_output_dir = "data/output/test"
            os.makedirs(test_output_dir, exist_ok=True)
            timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
            base_path = os.path.join(test_output_dir, f"test_viz_{timestamp}")
            
            # Generate different visualizations
            word_cloud_path = f"{base_path}_wordcloud.png"
            plot_word_cloud(" ".join([entry["content"]["text"] for entry in sample_data]), output_path=word_cloud_path)
            
            if any("category" in entry["content"] for entry in sample_data):
                category_path = f"{base_path}_categories.png"
                categories = {entry["content"]["category"]: 1 for entry in sample_data}
                plot_category_distribution(categories, output_path=category_path)
            
            length_path = f"{base_path}_lengths.png"
            lengths = [len(entry["content"]["text"]) for entry in sample_data]
            plot_length_distribution(lengths, output_path=length_path)
            
            freq_path = f"{base_path}_word_freq.png"
            all_text = " ".join([entry["content"]["text"] for entry in sample_data])
            from collections import Counter
            word_counts = Counter(all_text.split()).most_common(20)
            plot_word_frequency(word_counts, output_path=freq_path)
            
            print(f"{Fore.GREEN}[SUCCESS] Dataset visualizations created in {test_output_dir}{Style.RESET_ALL}")
            return True
        except Exception as e:
            print(f"{Fore.RED}[ERROR] Dataset visualization failed: {e}{Style.RESET_ALL}")
            return False
    except Exception as e:
        print(f"{Fore.RED}[ERROR] Dataset analysis failed: {e}{Style.RESET_ALL}")
        return False

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    parser = argparse.ArgumentParser(description="DekDataset - Test Data Integration")
    parser.add_argument("--test-cleaning", action="store_true", help="Test data cleaning functions")
    parser.add_argument("--test-analysis", action="store_true", help="Test dataset analysis functions")
    parser.add_argument("--test-all", action="store_true", help="Test all functions")
    args = parser.parse_args()
    
    if args.test_all or args.test_cleaning:
        test_data_cleaning()
        
    if args.test_all or args.test_analysis:
        test_dataset_analysis()
        
    if not (args.test_all or args.test_cleaning or args.test_analysis):
        print("No tests specified. Use --test-all, --test-cleaning, or --test-analysis")
        parser.print_help()

if __name__ == "__main__":
    print(f"{Fore.CYAN}=== DekDataset - Data Integration Tests ==={Style.RESET_ALL}")
    main()
