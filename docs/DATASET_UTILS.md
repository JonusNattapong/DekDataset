# DekDataset - Dataset Generation Tools

## üöÄ New Features

### Data Cleaning & Text Normalization

‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞ normalize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥:

```bash
# ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ö‡∏ö default (‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢ default)
python src/python/generate_dataset.py sentiment_analysis 100

# ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
python src/python/generate_dataset.py sentiment_analysis 100 --no-clean

# ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£ normalize ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
python src/python/generate_dataset.py sentiment_analysis 100 --disable-thai-norm

# ‡∏•‡∏ö emoji ‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©
python src/python/generate_dataset.py sentiment_analysis 100 --remove-emojis --remove-special-chars
```

[‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Data Cleaning](./DATA_CLEANING.md)

DekDataset ‡∏°‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞ normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥:
- ‡∏•‡∏ö‡πÅ‡∏ó‡πá‡∏Å HTML
- ‡∏•‡∏ö emoji
- Normalize ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏£‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡πÄ ‚Üí ‡πÅ)
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö medical_benchmark)

### ‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á Train/Valid/Test

‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏ö‡πà‡∏á dataset ‡πÄ‡∏õ‡πá‡∏ô train/validation/test ‡πÑ‡∏î‡πâ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥:
```bash
python src/python/generate_dataset.py medical_benchmark 500 --split --train-ratio 0.8 --valid-ratio 0.1 --test-ratio 0.1
```

### ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏à‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏° batch

‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô batch ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≤‡∏° batch:
- ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥

### ‡πÄ‡∏û‡∏¥‡πà‡∏° Custom Metadata

‡πÄ‡∏û‡∏¥‡πà‡∏° metadata ‡πÄ‡∏ä‡πà‡∏ô license, version, domain ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
```bash
python src/python/generate_dataset.py sentiment_analysis 100 --license "MIT" --version "2.0.0" --domain "social-media"
```

‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ JSON string ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö metadata ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô:
```bash
python src/python/generate_dataset.py summarization 50 --metadata '{"source": "news", "tags": ["Thai", "politics"]}'
```

### ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏¥‡∏°

‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß:

```bash
python src/python/generate_dataset.py medical_benchmark 100 --append data/output/auto-dataset-medical_benchmark-20250524-120000.jsonl
```

### ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÑ‡∏î‡πâ:
```bash
# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
python src/python/generate_dataset.py sentiment_analysis 100 --analyze

# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
python src/python/generate_dataset.py sentiment_analysis 100 --analyze --visualize

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
python src/python/generate_dataset.py sentiment_analysis 100 --analyze --analyze-output data/analysis/my-dataset
```
[‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Dataset Analysis](./DATASET_ANALYSIS.md)

### ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub

‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á:
```bash
# ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏¢‡∏±‡∏á Hugging Face Hub
python src/python/generate_dataset.py sentiment_analysis 100 --export-huggingface --hf-repo-id username/my-dataset

# ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô private repository
python src/python/generate_dataset.py sentiment_analysis 100 --export-huggingface --hf-repo-id username/my-dataset --hf-private
```
[‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Hugging Face Hub Integration](./HUGGINGFACE_HUB.md)

## üë®‚Äçüíª Unit Tests

DekDataset ‡∏°‡∏µ unit tests ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡πÄ‡∏ä‡πà‡∏ô clean_text, deduplicate_entries:
```bash
python src/python/test_dataset_utils.py
```

## üìö Command Line Arguments ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

```
usage: generate_dataset.py [-h] [--format {json,jsonl,csv,parquet}] [--import-vision IMPORT_VISION] [--source_lang SOURCE_LANG]
                          [--target_lang TARGET_LANG] [--lang LANG] [--no-clean] [--disable-thai-norm] [--license LICENSE]
                          [--version VERSION] [--domain DOMAIN] [--split] [--train-ratio TRAIN_RATIO] [--valid-ratio VALID_RATIO]
                          [--test-ratio TEST_RATIO] [--append APPEND] [--metadata METADATA]
                          task count

DekDataset: Generate high-quality dataset for NLP and other tasks

positional arguments:
  task                  Task name (e.g. sentiment_analysis, medical_benchmark)
  count                 Number of samples to generate

options:
  -h, --help            show this help message and exit
  --format {json,jsonl,csv,parquet}
                        Output format: json, jsonl, csv, parquet
  --import-vision IMPORT_VISION
                        Path to vision-animals-dataset-*.jsonl to import/validate/export
  --source_lang SOURCE_LANG
                        Source language (for translation task)
  --target_lang TARGET_LANG
                        Target language (for translation task)
  --lang LANG           Language code (for multilingual tasks, e.g. th, en, zh, hi)
  --no-clean            Skip data cleaning/normalization
  --disable-thai-norm   Disable Thai text normalization
  --license LICENSE     License for the dataset metadata
  --version VERSION     Version for the dataset metadata
  --domain DOMAIN       Domain for the dataset metadata (e.g. medical, education, finance)
  --split               Split dataset into train/valid/test
  --train-ratio TRAIN_RATIO
                        Train set ratio when splitting (default: 0.8)
  --valid-ratio VALID_RATIO
                        Validation set ratio when splitting (default: 0.1)
  --test-ratio TEST_RATIO
                        Test set ratio when splitting (default: 0.1)
  --append APPEND       Path to existing dataset file to append to (deduplicate across both)
  --metadata METADATA   Additional metadata as JSON string, e.g. '{"source": "custom", "tags": ["medical"]}'
```
