# ğŸ”¥ DekDataset

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![DeepSeek API](https://img.shields.io/badge/DeepSeek-API-FF6B6B?style=for-the-badge&logo=openai&logoColor=white)
![Mistral OCR](https://img.shields.io/badge/Mistral-OCR-4ECDC4?style=for-the-badge&logo=mistral&logoColor=white)
![License: MIT](https://img.shields.io/badge/License-MIT-2ECC71?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Production_Ready-27AE60?style=for-the-badge)

</div>

---

<div align="center">
  <h2>ğŸ¯ Advanced AI Dataset Generator for Thai & Multilingual Applications</h2>
  <p><i>Professional-grade synthetic data generation with OCR, NLP, Vision & Multi-modal capabilities</i></p>
  
  <b>âš¡ Fast â€¢ ğŸ¯ Accurate â€¢ ğŸ”§ Extensible â€¢ ğŸ“Š Production-Ready</b>
</div>

---

## ğŸŒŸ Overview

**DekDataset** is a comprehensive open-source framework for generating high-quality AI/ML datasets in Thai and multiple languages. Designed for both research and enterprise applications, it seamlessly integrates OCR document processing, NLP tasks, computer vision, and multi-modal data generation into a unified, scalable platform.

### ğŸ¯ Key Differentiators

- ğŸ” **Advanced OCR Integration**: Extract text from PDFs, images, and documents using Mistral OCR API
- ğŸ§  **AI-Powered Generation**: Leverage DeepSeek LLM for context-aware synthetic data creation  
- ğŸŒ **Thai Language Optimized**: Native support for Thai NLP tasks and cultural context
- ğŸ­ **Enterprise-Ready**: Robust error handling, batch processing, and production deployment features

---

## ğŸ“‘ Table of Contents

- [ğŸŒŸ Overview](#-overview)
- [âœ¨ Features](#-features)  
- [âš¡ Quick Start](#-quick-start)
- [ğŸ” OCR Document Processing](#-ocr-document-processing)
- [ğŸ› ï¸ How It Works](#ï¸-how-it-works)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ–¼ï¸ Example Use Cases](#ï¸-example-use-cases)
- [ğŸ§‘â€ğŸ’» Technical Details](#-technical-details)
- [ğŸ™ Credits & License](#-credits--license)

---

## âœ¨ Features

### ğŸ”§ Core Capabilities

- **ğŸ¯ Unified Task Schema**: Support for NLP, Vision, OCR, and Multi-modal tasks with centralized schema management
- **ğŸ¤– Automatic Prompting**: Generate optimized prompts for LLMs (DeepSeek, OpenAI, etc.) automatically
- **âš¡ Batch Generation**: Advanced batch processing with error recovery, quota management, and intelligent retry mechanisms
- **ğŸ“Š Data Validation & Metadata**: Complete validation, deduplication, enrichment, label balancing, and metadata export
- **ğŸ’¾ Flexible Output**: Export to JSONL, Parquet, Arrow, CSV formats compatible with HuggingFace, PyArrow, and Pandas

### ğŸ” Advanced OCR & Document Processing

- **ğŸ“„ PDF Processing**: Extract text from multi-page PDF documents using Poppler integration
- **ğŸ–¼ï¸ Image OCR**: Process JPG, PNG, JPEG images with high-accuracy text recognition
- **ğŸŒ URL Support**: Direct processing of remote documents and images via URLs
- **ğŸ¯ Context-Aware Generation**: Use extracted OCR text as context for LLM-based dataset creation

### ğŸŒ Web Integration & Media

- **ğŸ” Web Scraping**: Download images from Bing, DuckDuckGo, Pexels, Pixabay with metadata
- **ğŸ¨ AI Image Generation**: Integrated support for AI-generated images and captions
- **ğŸ“± Multi-modal**: Combine text, images, and metadata for comprehensive datasets

### ğŸ›¡ï¸ Enterprise Features

- **ğŸ”’ Robust & Reproducible**: Comprehensive error handling, logging, retry mechanisms, and fallback strategies
- **ğŸ“ˆ Scalable Architecture**: Support for large-scale batch processing and distributed generation
- **ğŸ”§ Extensible Design**: Easy task/schema customization via `tasks.json` or REST API
- **ğŸ“‹ Production Monitoring**: Built-in monitoring, logging, and performance tracking

---

## ğŸ“¦ Installation

1. Install Python dependencies:
```bash
pip install -r requirements.txt

# Install additional required packages
pip install playwright bs4 nest-asyncio crawl4ai pandas
```

2. Install Playwright browsers:
```bash
playwright install
```

3. Install Poppler for PDF processing:
```bash
# Windows
# Download and extract poppler-windows.zip from the repository
unzip poppler-windows.zip -d ./poppler-local

# Linux
sudo apt-get update && sudo apt-get install -y poppler-utils

# Mac
brew install poppler
```

4. Verify OCR Installation:
```bash
# Test OCR setup
python src/python/ocr_utils.py --test

# Test PDF processing
python src/python/ocr_utils.py --test-pdf sample.pdf
```

## âš¡ Quick Start Examples

### Social Media Comment Extraction

Extract comments from various social media platforms:

```bash
# Extract Twitter/X comments
python src/python/get_comments.py x https://x.com/username/status/123456 --max_results 100

# Extract Facebook comments
python src/python/get_comments.py facebook https://facebook.com/post/123456 --max_results 50

# Extract YouTube comments
python src/python/get_comments.py youtube https://youtube.com/watch?v=videoId --max_results 200
```

### Save & Export Options

Save extracted comments in different formats:

```bash
# Save as JSONL (default)
python src/python/get_comments.py x https://x.com/post/123 --output comments.jsonl

# Save as CSV
python src/python/get_comments.py x https://x.com/post/123 --format csv --output comments.csv

# Save with sentiment analysis
python src/python/get_comments.py x https://x.com/post/123 --sentiment
```

### Advanced Features

```bash
# Extract with spam filtering
python src/python/get_comments.py x https://x.com/post/123 --filter-spam

# Extract with both sentiment and spam filtering
python src/python/get_comments.py x https://x.com/post/123 --sentiment --filter-spam

# Extract from local file
python src/python/get_comments.py file path/to/comments.jsonl
```

## ğŸ” Troubleshooting

### Common Issues

1. Playwright Installation
```bash
# If you see browser-related errors, try:
playwright install
playwright install chromium
```

2. SSL Certificate Errors
```bash
# Set environment variable to ignore SSL
set PYTHONHTTPSVERIFY=0  # Windows
export PYTHONHTTPSVERIFY=0  # Linux/Mac
```

3. Rate Limiting
```bash
# Use delay between requests
python src/python/get_comments.py x https://x.com/post/123 --delay 5
```

### Platform-Specific Notes

#### Twitter/X
- Uses Playwright for extraction with fallback to Crawl4AI
- Handles both old twitter.com and new x.com URLs
- Supports thread extraction

#### Facebook
- Requires public posts
- May need longer delays between requests
- Supports post and comment extraction

#### YouTube
- Requires yt-dlp package
- Supports both video IDs and full URLs
- Can extract comments and replies

## ğŸŒŸ Usage

### 1. Installation & Setup

```bash
# Clone repository
git clone https://github.com/yourusername/DekDataset.git
cd DekDataset

# Install dependencies
pip install -r requirements.txt

# Install additional required packages
pip install playwright bs4 nest-asyncio crawl4ai pandas

# Install Playwright browsers
playwright install
```

### 2. Configure API Keys

Create a `.env` file in the project root:

```env
# Required: DeepSeek API for LLM generation
DEEPSEEK_API_KEY=your_deepseek_api_key

# Required: Mistral API for OCR processing
MISTRAL_API_KEY=your_mistral_api_key

# Optional: Image download services
PEXELS_API_KEY=your_pexels_api_key
PIXABAY_API_KEY=your_pixabay_api_key
```

### 3. Basic Dataset Generation

```bash
# Generate NLP dataset
python src/python/generate_dataset.py sentiment_analysis 100 --format jsonl

# Generate dataset from PDF document
python src/python/generate_dataset.py primary_school_knowledge 50 --input-file document.pdf

# Generate with custom settings
python src/python/generate_dataset.py medical_text_summarization 25 --delay 2 --format parquet

# Generate with DeepSeek API
python src/python/generate_dataset.py --model deepseek-chat --temperature 0.7 --task custom_task 100

# Process large PDF documents
python src/python/ocr_utils.py --input large_document.pdf --batch-size 10 --output extracted_text.txt

# Generate dataset with specific language
python src/python/generate_dataset.py sentiment_analysis 100 --language th --format jsonl

# Export and merge datasets
python src/python/data_utils.py merge dataset1.jsonl dataset2.jsonl --output merged.jsonl
python src/python/data_utils.py convert input.jsonl --format parquet --output final.parquet
```

### 4. Advanced Usage

```bash
# Start task definitions API server
python src/python/task_definitions_api.py

# DeepSeek API Advanced Usage
# Generate with specific model parameters
python src/python/generate_dataset.py custom_task 100 \
  --model deepseek-chat \
  --temperature 0.7 \
  --top-p 0.9 \
  --frequency-penalty 0.3

# Batch generation with error recovery
python src/python/generate_dataset.py sentiment_analysis 1000 \
  --batch-size 50 \
  --retry-attempts 3 \
  --delay 2

# Dataset Management
# Convert between formats
python src/python/data_utils.py convert dataset.jsonl \
  --format parquet \
  --compression snappy \
  --output processed.parquet

# Filter and clean dataset
python src/python/data_utils.py clean dataset.jsonl \
  --remove-duplicates \
  --min-length 10 \
  --language th \
  --output cleaned.jsonl

# Merge multiple datasets
python src/python/data_utils.py merge \
  dataset1.jsonl dataset2.jsonl dataset3.jsonl \
  --output merged.jsonl \
  --format parquet

# Split dataset
python src/python/data_utils.py split dataset.jsonl \
  --train 0.8 \
  --val 0.1 \
  --test 0.1

# Dataset Analysis
python src/python/data_utils.py analyze dataset.jsonl \
  --show-stats \
  --plot-distributions \
  --export-report report.pdf
```

---

## ğŸ” OCR Document Processing

### Supported Formats

- **ğŸ“„ PDF Documents**: Multi-page PDFs with automatic page-by-page processing
- **ğŸ–¼ï¸ Images**: JPG, PNG, JPEG files with high-accuracy text extraction
- **ğŸŒ Remote Files**: Direct URL processing for online documents and images

### OCR Workflow Examples

#### Extract Text Only

```bash
# Extract from local PDF
python src/python/generate_dataset.py --input-file document.pdf

# Extract from image
python src/python/generate_dataset.py --input-file image.png

# Extract from URL
python src/python/generate_dataset.py --input-file https://example.com/document.pdf
```

#### Generate Dataset from Documents

```bash
# Create sentiment analysis dataset from PDF
python src/python/generate_dataset.py sentiment_analysis 20 --input-file textbook.pdf

# Generate medical dataset from research paper
python src/python/generate_dataset.py medical_text_summarization 15 --input-file research_paper.pdf

# Create Q&A dataset from educational content
python src/python/generate_dataset.py primary_school_knowledge 30 --input-file educational_material.png
```

### OCR Technical Features

- **ğŸ”§ Automatic Poppler Integration**: Built-in PDF processing without system dependencies
- **ğŸ¯ High-Accuracy Text Extraction**: Mistral OCR API with advanced text recognition
- **ğŸ“Š Context-Aware Processing**: Extracted text feeds directly into LLM generation pipeline
- **âš¡ Batch Document Processing**: Handle multiple documents and large files efficiently

---

## ğŸ› ï¸ How It Works

### Generation Pipeline

1. **ğŸ“‹ Task Selection**: Choose from predefined tasks or create custom schemas
2. **ğŸ” Document Processing**: (Optional) Extract text from PDFs/images using OCR
3. **ğŸ¤– Prompt Generation**: Automatically generate optimized prompts for LLM
4. **âš¡ Batch Generation**: Create synthetic data in intelligent batches with error recovery
5. **âœ… Validation & Processing**: Validate, deduplicate, enrich, and balance generated data
6. **ğŸ’¾ Export & Metadata**: Export to multiple formats with comprehensive metadata

### Architecture Overview

```mermaid
graph TD
    A[Input Documents/Tasks] --> B[OCR Processing]
    B --> C[Context Extraction]
    C --> D[LLM Prompt Generation]
    D --> E[Batch Generation]
    E --> F[Data Validation]
    F --> G[Export & Metadata]
    G --> H[Multiple Output Formats]
```

---

## ğŸ“ Project Structure

```text
DekDataset/
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ ğŸ¦€ main.rs, models.rs, api_client.rs, generator.rs, banner.rs
â”‚   â””â”€â”€ ğŸ python/
â”‚       â”œâ”€â”€ generate_dataset.py      # Main generation script
â”‚       â”œâ”€â”€ ocr_utils.py            # OCR processing module
â”‚       â”œâ”€â”€ task_definitions.py     # Task schema management
â”‚       â””â”€â”€ task_definitions_api.py # REST API server
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ output/                     # Generated datasets
â”‚   â””â”€â”€ pdf/                        # Sample PDF documents
â”œâ”€â”€ ğŸ“ poppler-local/               # Portable PDF processing
â”œâ”€â”€ ğŸ“ docs/                        # Documentation
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â””â”€â”€ ğŸ“„ README.md                    # This file
```

---

## ğŸ–¼ï¸ Example Use Cases

### 1. Medical AI Dataset Creation

```bash
# Generate comprehensive medical benchmark dataset
python src/python/generate_dataset.py medical_benchmark 1000 --format jsonl --delay 1

# Create medical Q&A from research papers
python src/python/generate_dataset.py medical_text_summarization 200 --input-file medical_research.pdf
```

**Output**: High-quality medical datasets for training AI models, medical Q&A systems, or clinical decision support tools.

### 2. Educational Content Generation

```bash
# Thai primary school knowledge base
python src/python/generate_dataset.py primary_school_knowledge 500 --input-file thai_textbook.pdf

# Generate educational Q&A pairs
python src/python/generate_dataset.py qa_generation 300 --input-file educational_content.pdf
```

**Output**: Educational datasets for Thai language learning applications, automated tutoring systems, and knowledge assessment tools.

### 3. Thai NLP Model Training

```bash
# Sentiment analysis dataset in Thai
python src/python/generate_dataset.py sentiment_analysis 2000 --format parquet

# Text classification for Thai content
python src/python/generate_dataset.py text_classification 1500 --delay 2
```

**Output**: Large-scale Thai language datasets for training sentiment analysis, text classification, and other NLP models.

### 4. Document Processing Automation

```bash
# Process multiple PDF documents
python src/python/generate_dataset.py document_summarization 100 --input-file legal_documents.pdf

# Extract and analyze business reports
python src/python/generate_dataset.py business_analysis 75 --input-file quarterly_report.pdf
```

**Output**: Automated document analysis and summarization datasets for enterprise applications.

---

## ğŸ§‘â€ğŸ’» Technical Details

### Technology Stack

- **ğŸ Python 3.10+**: Core processing and API integration
- **ğŸ¦€ Rust Components**: High-performance data processing modules
- **ğŸ¤– DeepSeek API**: Advanced LLM for synthetic data generation
- **ğŸ” Mistral OCR**: State-of-the-art optical character recognition
- **ğŸ“„ Poppler**: Robust PDF processing and conversion
- **âš¡ FastAPI**: REST API for task management and automation

### Key Components

- **ğŸ”§ OCR Engine**: Mistral API integration with local Poppler support
- **ğŸ“Š Data Pipeline**: Robust batch processing with error recovery
- **ğŸ¯ Task Management**: Flexible schema system with custom task support
- **ğŸ’¾ Export System**: Multi-format output with metadata preservation
- **ğŸ›¡ï¸ Error Handling**: Comprehensive logging and retry mechanisms

### Performance & Scalability

- **âš¡ Batch Processing**: Intelligent batching for optimal API usage
- **ğŸ”„ Error Recovery**: Automatic retry with exponential backoff
- **ğŸ“ˆ Scalable Architecture**: Support for distributed processing
- **ğŸ“Š Monitoring**: Built-in performance tracking and logging

### Integration Capabilities

- **ğŸ¤— HuggingFace**: Direct compatibility with Datasets and Transformers
- **ğŸ“Š PyArrow/Pandas**: Native support for data analysis workflows
- **â˜ï¸ Cloud Deployment**: Docker and cloud-ready architecture
- **ğŸ”Œ API Integration**: RESTful API for external system integration

---

## ğŸ™ Credits & License

### Acknowledgments

- **ğŸ¢ DeepSeek**: Advanced LLM capabilities and API integration
- **ğŸ” Mistral AI**: High-quality OCR processing and text extraction
- **ğŸ¤— HuggingFace**: Ecosystem integration and dataset standards
- **ğŸ“Š Apache Arrow**: High-performance data processing and storage
- **ğŸŒ Open Source Community**: Various libraries and tools that make this project possible

### License & Usage

- **ğŸ“„ License**: MIT License - see [LICENSE](LICENSE) for details
- **ğŸ’¼ Commercial Use**: Permitted under MIT license terms
- **ğŸ”§ Contributions**: Welcome via pull requests and issues
- **ğŸ“ Support**: Community support through GitHub issues

### Project Information

- **ğŸ‘¨â€ğŸ’» Developed by**: ZOMBIT Team
- **ğŸŒ Repository**: [github.com/JonusNattapong/DekDataset](https://github.com/JonusNattapong/DekDataset)
- **ğŸ“§ Contact**: zombitx64@gmail.com
- **ğŸ¯ Version**: 2025.05 - Production Ready
- **ğŸ·ï¸ Tags**: AI, ML, Dataset, Thai NLP, OCR, Synthetic Data, DeepSeek, Python

---

<div align="center">

**â­ Star this repository if you find it useful!**

[![GitHub stars](https://img.shields.io/github/stars/JonusNattapong/DekDataset?style=social)](https://github.com/JonusNattapong/DekDataset/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/JonusNattapong/DekDataset?style=social)](https://github.com/JonusNattapong/DekDataset/network/members)
[![GitHub issues](https://img.shields.io/github/issues/JonusNattapong/DekDataset)](https://github.com/JonusNattapong/DekDataset/issues)

</div>
